{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaurang\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n",
      "C:\\Users\\gaurang\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\gaurang\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import argmax\n",
    "from boruta import BorutaPy\n",
    "from tpot import TPOTRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', Cleaner()), ('normalizer', Normalizer()), ('train_test_split', SplitData()), ('features', BorutaPy(alpha=0.05,\n",
       "     estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decreas...timators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame.from_csv(\"https://raw.githubusercontent.com/LuisM78/Appliances-energy-prediction-data/master/energydata_complete.csv\")\n",
    "\n",
    "class Cleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, performs cleaning if needed and returns cleaned dataframe\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def seconds(self, x):\n",
    "        sec = x.hour*3600+x.minute*60+x.second\n",
    "        return sec\n",
    "    \n",
    "    def day_week(self, z):\n",
    "        a=[]\n",
    "        for y in z:\n",
    "            if y == 0:\n",
    "                a.append('Monday')\n",
    "            elif y == 1:\n",
    "                a.append('Tuesday')\n",
    "            elif y == 2:\n",
    "                a.append('Wednesday')\n",
    "            elif y == 3:\n",
    "                a.append('Thrusday')\n",
    "            elif y == 4:\n",
    "                a.append('Friday')\n",
    "            elif y == 5:\n",
    "                a.append('Saturday')\n",
    "            elif y == 6:\n",
    "                a.append('Sunday')\n",
    "        return a\n",
    "    \n",
    "    def week(self, x):\n",
    "        a=[]\n",
    "        for y in x:\n",
    "            if y == 'Saturday' or y == 'Sunday':\n",
    "                a.append('weekend')\n",
    "            else:\n",
    "                a.append('weekday')\n",
    "        return a\n",
    "    \n",
    "    def one_hot_encode(self, Data):\n",
    "        label_encoder = LabelEncoder()\n",
    "        int_encoded = label_encoder.fit_transform(Data['week_status'])\n",
    "        int_encoded_day = label_encoder.fit_transform(Data['Day_Status'])\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        int_encoded = int_encoded.reshape(len(int_encoded), 1)\n",
    "        int_encoded_day = int_encoded_day.reshape(len(int_encoded_day), 1)\n",
    "        newWeek = onehot_encoder.fit_transform(int_encoded)\n",
    "        newDay = onehot_encoder.fit_transform(int_encoded_day)\n",
    "        # new2 = label_encoder.inverse_transform([argmax(new[len(new)-1, :])])\n",
    "        Data.drop(['week_status', 'Day_Status'], axis=1, inplace=True)\n",
    "        Data['Friday'] = pd.Series(newDay[:,0], index=Data.index)\n",
    "        Data['Monday'] = pd.Series(newDay[:,1], index=Data.index)\n",
    "        Data['Saturday'] = pd.Series(newDay[:,2], index=Data.index)\n",
    "        Data['Sunday'] = pd.Series(newDay[:,3], index=Data.index)\n",
    "        Data['Thursday'] = pd.Series(newDay[:,4], index=Data.index)\n",
    "        Data['Tuesday'] = pd.Series(newDay[:,5], index=Data.index)\n",
    "        Data['Wednesday'] = pd.Series(newDay[:,6], index=Data.index)\n",
    "        Data['WeekDay'] = pd.Series(newWeek[:,0], index=Data.index)\n",
    "        Data['Weekend'] = pd.Series(newWeek[:,1], index=Data.index)\n",
    "        return Data\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"Adding the columns Day_Status, week_status and Num_sec_midnight\"\"\"\n",
    "        \n",
    "        df['Num_sec_midnight']=self.seconds(df.index)\n",
    "        z = df.index.dayofweek\n",
    "        df['Day_Status'] = z\n",
    "        df['Day_Status'] = self.day_week(df.Day_Status)\n",
    "        df['week_status'] = self.week(df.Day_Status)\n",
    "        \n",
    "        \"\"\"Performing one hot encoding on week_status and day_status columns\"\"\"\n",
    "        df=self.one_hot_encode(df)\n",
    "        return df\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class Normalizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"Performs Normalization on all the columns except for Appliances\"\"\"\n",
    "        for j in range(1, len(df.columns)-1,1):\n",
    "            df.iloc[:,[j]] = (df.iloc[:,[j]] - df.iloc[:,[j]].mean())/df.iloc[:,[j]].std()\n",
    "        df.to_csv(\"normalized.csv\")\n",
    "        return df\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class SplitData(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        y = df['Appliances']\n",
    "        df4 = df.iloc[:,1:]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df4, y, test_size=0.25)\n",
    "        train = X_train.join(y_train)\n",
    "        test = X_test.join(y_test)\n",
    "        train.to_csv(\"train.csv\")\n",
    "        test.to_csv(\"test.csv\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "class transformData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        return df\n",
    "        \n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "pipeline = Pipeline([(\"cleaner\", Cleaner()),\n",
    "                     (\"normalizer\", Normalizer()),\n",
    "                     (\"train_test_split\", SplitData()),\n",
    "                     (\"features\", BorutaPy(RandomForestRegressor())),\n",
    "                     (\"transofrm_data\", transformData()),\n",
    "                     (\"estimator\", RandomForestRegressor())\n",
    "                    ])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cleaner', Cleaner()),\n",
       " ('normalizer', Normalizer()),\n",
       " ('train_test_split', SplitData()),\n",
       " ('features', BorutaPy(alpha=0.05,\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       max_iter=100, n_estimators=1000, perc=100, random_state=None,\n",
       "       two_step=True, verbose=0)),\n",
       " ('transofrm_data', transformData()),\n",
       " ('estimator',\n",
       "  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = Cleaner().transform(dataset)\n",
    "dataset = Normalizer().transform(dataset)\n",
    "X_train, X_test, y_train, y_test = SplitData().transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = [{'Boruta_rf__n_estimators': [100,200,300]}]\n",
    "grid = GridSearchCV(pipeline, cv = 10, param_grid=param_grid, n_jobs=2,verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
